(window.webpackJsonp=window.webpackJsonp||[]).push([[344],{1091:function(t,e,a){t.exports=a.p+"assets/img/bulk-load-s3.ecb657db.png"},2392:function(t,e,a){"use strict";a.r(e);var r=a(0),o=Object(r.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"snowflake-bulk-load-from-external-stage-action"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#snowflake-bulk-load-from-external-stage-action"}},[t._v("#")]),t._v(" Snowflake - Bulk Load from external stage action")]),t._v(" "),r("p",[t._v("Load a file from an external stage Amazon S3 bucket as an external source into a target table. This action uses the "),r("a",{attrs:{href:"https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-table.html#output",target:"_blank",rel:"noopener noreferrer"}},[t._v("COPY"),r("OutboundLink")],1),t._v(" command to load data directly from an external source to a target table.")]),t._v(" "),r("p",[t._v("This action will execute the load and wait for completion before moving onto the next step. Load time depends on the size of source file, number of columns, additional validation in the target table and network speed (faster if loading data from S3 to an AWS-deployed Snowflake instance). "),r("strong",[t._v("1 GB")]),t._v(" CSV file with "),r("strong",[t._v("30 columns")]),t._v(" and "),r("strong",[t._v("3 million rows")]),t._v(" will take 60 seconds.")]),t._v(" "),r("p",[t._v("The source file can contain data in CSV format, JSON, PARQUET and "),r("a",{attrs:{href:"https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-table.html#format-type-options-formattypeoptions",target:"_blank",rel:"noopener noreferrer"}},[t._v("other semi-structured file types"),r("OutboundLink")],1),t._v(".")]),t._v(" "),r("h2",{attrs:{id:"load-data-from-an-amazon-s3-bucket-to-a-table"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#load-data-from-an-amazon-s3-bucket-to-a-table"}},[t._v("#")]),t._v(" Load data from an Amazon S3 bucket to a table")]),t._v(" "),r("p",[r("DocImage",{attrs:{src:a(1091),alt:"Bulk Load from Amazon S3 action",width:"2214",height:"1584"}}),t._v(" "),r("em",[t._v("Bulk Load from Amazon S3 action")])],1),t._v(" "),r("h3",{attrs:{id:"input-fields"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#input-fields"}},[t._v("#")]),t._v(" Input fields")]),t._v(" "),r("table",{staticClass:"unchanged rich-diff-level-one"},[r("thead",[r("tr",[r("th",{attrs:{width:"25%"}},[t._v("Input field")]),t._v(" "),r("th",[t._v("Description")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Table")]),t._v(" "),r("td",[t._v("\n        Select a target table to load data into. Typically, this is a staging table for loading data. Subsequently, rows in this table is merged into your production table.\n      ")])]),t._v(" "),r("tr",[r("td",[t._v("Stage")]),t._v(" "),r("td",[r("p",[t._v("Select an existing external stage that points to an Amazon S3 bucket. If a file is not specified in this stage, all new files will be loaded. This external stage contains information about file location, AWS credentials, encryption and file format details.")]),t._v(" "),r("p",[t._v("Learn how to create an "),r("a",{attrs:{href:"https://docs.snowflake.net/manuals/user-guide/data-load-s3-create-stage.html"}},[t._v("S3 external stage")]),t._v(".")])])])])]),t._v(" "),r("h3",{attrs:{id:"output-fields"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#output-fields"}},[t._v("#")]),t._v(" Output fields")]),t._v(" "),r("table",{staticClass:"unchanged rich-diff-level-one"},[r("thead",[r("tr",[r("th",{attrs:{width:"25%"}},[t._v("Output field")]),t._v(" "),r("th",[t._v("Description")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("Bucket URL")]),t._v(" "),r("td",[t._v("Relative path and name of the source file."),r("br"),t._v("Example: "),r("b",[t._v("s3://bucket-name/parent_folder/file_name.csv")])])]),t._v(" "),r("tr",[r("td",[t._v("Status")]),t._v(" "),r("td",[r("table",{staticClass:"unchanged rich-diff-level-one"},[r("thead",[r("tr",[r("th",[t._v("Values")]),t._v(" "),r("th",[t._v("Description")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("LOADED")]),t._v(" "),r("td",[t._v("All rows successfully loaded.")])]),t._v(" "),r("tr",[r("td",[t._v("LOAD FAILED")]),t._v(" "),r("td",[t._v("Unsuccessful load. Entire data file was not loaded.")])]),t._v(" "),r("tr",[r("td",[t._v("PARTIALLY LOADED")]),t._v(" "),r("td",[t._v("Unsuccessful load. Data file was partially loaded. Use "),r("b",[t._v("First error line")]),t._v(" to find out exactly where the load failed.")])])])])])]),t._v(" "),r("tr",[r("td",[t._v("Rows parsed")]),t._v(" "),r("td",[t._v("Number of rows read from the source file.")])]),t._v(" "),r("tr",[r("td",[t._v("Rows loaded")]),t._v(" "),r("td",[t._v("Number of rows successfully loaded from the source file into target table.")])]),t._v(" "),r("tr",[r("td",[t._v("Error limit")]),t._v(" "),r("td",[t._v("If the number of errors reaches this limit, then abort the load. This is typically "),r("b",[t._v("0")]),t._v(", meaning that the load will abort on the first error.")])]),t._v(" "),r("tr",[r("td",[t._v("Errors seen")]),t._v(" "),r("td",[t._v("Number of rows with error in the source file.")])]),t._v(" "),r("tr",[r("td",[t._v("First error")]),t._v(" "),r("td",[t._v("Error details of the first error in the source file.")])]),t._v(" "),r("tr",[r("td",[t._v("First error line")]),t._v(" "),r("td",[t._v("Line number of the first row that caused an error.")])]),t._v(" "),r("tr",[r("td",[t._v("First error character")]),t._v(" "),r("td",[t._v("Position of the first character that caused an error.")])]),t._v(" "),r("tr",[r("td",[t._v("First error column name")]),t._v(" "),r("td",[t._v("Column name where the first error occurred.")])])])])])}),[],!1,null,null,null);e.default=o.exports}}]);